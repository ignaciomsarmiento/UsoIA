{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQg5PxhPSh13"
      },
      "source": [
        "<div >\n",
        "<img src = \"figs/banner.png\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2evzlHn-Sh15"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/UsoIA/blob/main/01_Intro_Notebook_LLM.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYaHPz2MSh18"
      },
      "source": [
        "# Chat GPT: Entendiendo los Modelos de Lenguaje y su Funcionamiento Interno\n",
        "\n",
        "Chat GPT es lo que llamamos un **modelo de lenguaje**, ya que se encarga de modelar secuencias de palabras, caracteres o, de forma más general, **tokens**. Su principal habilidad radica en comprender cómo las palabras se relacionan y siguen unas a otras en un idioma, en este caso, el inglés.\n",
        "\n",
        "Desde su perspectiva, lo que realmente hace es **completar secuencias**: tú le proporcionas el inicio de una secuencia y el modelo genera el resto basándose en patrones que ha aprendido durante su entrenamiento. Por esta razón, se clasifica como un modelo de lenguaje.\n",
        "\n",
        "Sin embargo, más allá de esta funcionalidad básica, es importante explorar cómo funciona \"bajo el capó\" Chat GPT, es decir, entender los componentes internos que hacen posible su desempeño. La base de este modelo es una red neuronal que modela la secuencia de palabras, y su funcionamiento se deriva de un artículo científico clave titulado **\"Attention is All You Need\"** publicado en 2017. Este trabajo marcó un antes y un después en la inteligencia artificial al introducir la **arquitectura Transformer**, la piedra angular de sistemas como Chat GPT.\n",
        "\n",
        "<div >\n",
        "<img src = \"figs/transformer.png\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1kU2pSVSh19"
      },
      "source": [
        "El término GPT significa **\"Generative Pre-trained Transformer\"** (Transformer Generativo Preentrenado), destacando los tres elementos clave que lo definen:\n",
        "\n",
        "1. **Generative (Generativo):** Es capaz de generar texto de forma autónoma y coherente.\n",
        "2. **Pre-trained (Preentrenado):** Ha sido entrenado previamente con grandes cantidades de datos para entender patrones lingüísticos.\n",
        "3. **Transformer:** Utiliza la arquitectura Transformer, que basa su eficacia en un mecanismo conocido como **atención** para procesar y modelar secuencias de texto con gran precisión.\n",
        "\n",
        "Este tutorial se centrará en desglosar cómo la arquitectura Transformer hace posible que Chat GPT modele las secuencias de lenguaje y produzca respuestas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bx8J5HhE2QC9"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "\n",
        "# read it in to inspect it\n",
        "url = 'https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt'\n",
        "with urllib.request.urlopen(url) as f:\n",
        "    text = f.read().decode('utf-8')  # Decode the bytes to string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXwAy8J0863x",
        "outputId": "d4dd456f-124e-4477-819e-ac3bb7da2d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1038397\n"
          ]
        }
      ],
      "source": [
        "# print el numero de caracters unicos en text\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwfSa3L3Sh2A",
        "outputId": "ae431d87-cee8-43bf-d984-8e11f385296a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON QUIJOTE DE LA MANCHA\n",
            "Miguel de Cervantes Saavedra\n",
            "\n",
            "PRIMERA PARTE\n",
            "CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha\n",
            "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor. Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lentejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda. El resto della concluían sayo de velarte, calzas de velludo para las fiestas con sus pantuflos de lo mismo, los días de entre semana se honraba con su vellori de lo más fino. Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años, era de complexión recia, sec\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFOcXQ_m83hQ",
        "outputId": "e66c63cf-db1b-4e3a-8dec-994b2a8cc8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-.0123456789:;<?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijlmnopqrstuvxyz¡«»¿̀́̃̈–‘’“”\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ep7KcaZzaM2",
        "outputId": "2527152d-ff07-46b2-ccbf-76332194cc36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89\n"
          ]
        }
      ],
      "source": [
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7GAITvvGzucc"
      },
      "outputs": [],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xpivI8iz8Zc",
        "outputId": "c32f2ca5-9de0-4e6f-df17-d38b9b8b8ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31, 65, 62, 52, 1, 27, 65, 64]\n"
          ]
        }
      ],
      "source": [
        "print(encode(\"Hola Don\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0ULI3STT0Td3"
      },
      "outputs": [],
      "source": [
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmnShMX80cCd",
        "outputId": "fca920ad-3bc9-4987-eb3a-fadd2f810e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola Don\n"
          ]
        }
      ],
      "source": [
        "print(decode(encode(\"Hola Don\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uuUnG1bt1XF-"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83j8mJYU1lWX",
        "outputId": "9fb8f81b-9600-4646-824d-780da6800060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1038397]) torch.int64\n",
            "tensor([27, 38, 37,  1, 40, 44, 32, 33, 38, 43, 28,  1, 27, 28,  1, 35, 24,  1,\n",
            "        36, 24, 37, 26, 31, 24,  0, 36, 60, 58, 71, 56, 62,  1, 55, 56,  1, 26,\n",
            "        56, 68, 72, 52, 64, 70, 56, 69,  1, 42, 52, 52, 72, 56, 55, 68, 52,  0,\n",
            "         0, 39, 41, 32, 36, 28, 41, 24,  1, 39, 24, 41, 43, 28,  0, 26, 24, 39,\n",
            "        32, 81, 43, 44, 35, 38,  1, 11, 20,  1, 40, 71, 56,  1, 70, 68, 52, 70,\n",
            "        52,  1, 55, 56,  1, 62, 52,  1, 54, 65, 64, 55, 60, 54, 60, 65, 81, 64,\n",
            "         1, 74,  1, 56, 61, 56, 68, 54, 60, 54, 60, 65,  1, 55, 56, 62,  1, 57,\n",
            "        52, 63, 65, 69, 65,  1, 59, 60, 55, 52, 62, 58, 65,  1, 27,  9,  1, 40,\n",
            "        71, 60, 61, 65, 70, 56,  1, 55, 56,  1, 62, 52,  1, 36, 52, 64, 54, 59,\n",
            "        52,  0, 28, 64,  1, 71, 64,  1, 62, 71, 58, 52, 68,  1, 55, 56,  1, 62,\n",
            "        52,  1, 36, 52, 64, 54, 59, 52,  7,  1, 55, 56,  1, 54, 71, 74, 65,  1,\n",
            "        64, 65, 63, 53, 68, 56,  1, 64, 65,  1, 67, 71, 60, 56, 68, 65,  1, 52,\n",
            "        54, 65, 68, 55, 52, 68, 63, 56,  7,  1, 64, 65,  1, 59, 52,  1, 63, 71,\n",
            "        54, 59, 65,  1, 70, 60, 56, 63, 66, 65,  1, 67, 71, 56,  1, 72, 60, 72,\n",
            "        60, 81, 52,  1, 71, 64,  1, 59, 60, 55, 52, 62, 58, 65,  1, 55, 56,  1,\n",
            "        62, 65, 69,  1, 55, 56,  1, 62, 52, 64, 75, 52,  1, 56, 64,  1, 52, 69,\n",
            "        70, 60, 62, 62, 56, 68, 65,  7,  1, 52, 55, 52, 68, 58, 52,  1, 52, 64,\n",
            "        70, 60, 58, 71, 52,  7,  1, 68, 65, 54, 60, 81, 64,  1, 57, 62, 52, 54,\n",
            "        65,  1, 74,  1, 58, 52, 62, 58, 65,  1, 54, 65, 68, 68, 56, 55, 65, 68,\n",
            "         9,  1, 44, 64, 52,  1, 65, 62, 62, 52,  1, 55, 56,  1, 52, 62, 58, 65,\n",
            "         1, 63, 52, 81, 69,  1, 72, 52, 54, 52,  1, 67, 71, 56,  1, 54, 52, 68,\n",
            "        64, 56, 68, 65,  7,  1, 69, 52, 62, 66, 60, 54, 65, 81, 64,  1, 62, 52,\n",
            "        69,  1, 63, 52, 81, 69,  1, 64, 65, 54, 59, 56, 69,  7,  1, 55, 71, 56,\n",
            "        62, 65, 69,  1, 74,  1, 67, 71, 56, 53, 68, 52, 64, 70, 65, 69,  1, 62,\n",
            "        65, 69,  1, 69, 52, 81, 53, 52, 55, 65, 69,  7,  1, 62, 56, 64, 70, 56,\n",
            "        61, 52, 69,  1, 62, 65, 69,  1, 72, 60, 56, 68, 64, 56, 69,  7,  1, 52,\n",
            "        62, 58, 71, 81, 64,  1, 66, 52, 62, 65, 63, 60, 64, 65,  1, 55, 56,  1,\n",
            "        52, 64, 82, 52, 55, 60, 55, 71, 68, 52,  1, 62, 65, 69,  1, 55, 65, 63,\n",
            "        60, 64, 58, 65, 69,  7,  1, 54, 65, 64, 69, 71, 63, 60, 81, 52, 64,  1,\n",
            "        62, 52, 69,  1, 70, 68, 56, 69,  1, 66, 52, 68, 70, 56, 69,  1, 55, 56,\n",
            "         1, 69, 71,  1, 59, 52, 54, 60, 56, 64, 55, 52,  9,  1, 28, 62,  1, 68,\n",
            "        56, 69, 70, 65,  1, 55, 56, 62, 62, 52,  1, 54, 65, 64, 54, 62, 71, 60,\n",
            "        81, 52, 64,  1, 69, 52, 74, 65,  1, 55, 56,  1, 72, 56, 62, 52, 68, 70,\n",
            "        56,  7,  1, 54, 52, 62, 75, 52, 69,  1, 55, 56,  1, 72, 56, 62, 62, 71,\n",
            "        55, 65,  1, 66, 52, 68, 52,  1, 62, 52, 69,  1, 57, 60, 56, 69, 70, 52,\n",
            "        69,  1, 54, 65, 64,  1, 69, 71, 69,  1, 66, 52, 64, 70, 71, 57, 62, 65,\n",
            "        69,  1, 55, 56,  1, 62, 65,  1, 63, 60, 69, 63, 65,  7,  1, 62, 65, 69,\n",
            "         1, 55, 60, 81, 52, 69,  1, 55, 56,  1, 56, 64, 70, 68, 56,  1, 69, 56,\n",
            "        63, 52, 64, 52,  1, 69, 56,  1, 59, 65, 64, 68, 52, 53, 52,  1, 54, 65,\n",
            "        64,  1, 69, 71,  1, 72, 56, 62, 62, 65, 68, 60,  1, 55, 56,  1, 62, 65,\n",
            "         1, 63, 52, 81, 69,  1, 57, 60, 64, 65,  9,  1, 43, 56, 64, 60, 81, 52,\n",
            "         1, 56, 64,  1, 69, 71,  1, 54, 52, 69, 52,  1, 71, 64, 52,  1, 52, 63,\n",
            "        52,  1, 67, 71, 56,  1, 66, 52, 69, 52, 53, 52,  1, 55, 56,  1, 62, 65,\n",
            "        69,  1, 54, 71, 52, 68, 56, 64, 70, 52,  7,  1, 74,  1, 71, 64, 52,  1,\n",
            "        69, 65, 53, 68, 60, 64, 52,  1, 67, 71, 56,  1, 64, 65,  1, 62, 62, 56,\n",
            "        58, 52, 53, 52,  1, 52,  1, 62, 65, 69,  1, 72, 56, 60, 64, 70, 56,  7,\n",
            "         1, 74,  1, 71, 64,  1, 63, 65, 75, 65,  1, 55, 56,  1, 54, 52, 63, 66,\n",
            "        65,  1, 74,  1, 66, 62, 52, 75, 52,  7,  1, 67, 71, 56,  1, 52, 69, 60,\n",
            "        81,  1, 56, 64, 69, 60, 62, 62, 52, 53, 52,  1, 56, 62,  1, 68, 65, 54,\n",
            "        60, 81, 64,  1, 54, 65, 63, 65,  1, 70, 65, 63, 52, 53, 52,  1, 62, 52,\n",
            "         1, 66, 65, 55, 52, 55, 56, 68, 52,  9,  1, 29, 68, 60, 69, 52, 53, 52,\n",
            "         1, 62, 52,  1, 56, 55, 52, 55,  1, 55, 56,  1, 64, 71, 56, 69, 70, 68,\n",
            "        65,  1, 59, 60, 55, 52, 62, 58, 65,  1, 54, 65, 64,  1, 62, 65, 69,  1,\n",
            "        54, 60, 64, 54, 71, 56, 64, 70, 52,  1, 52, 64, 82, 65, 69,  7,  1, 56,\n",
            "        68, 52,  1, 55, 56,  1, 54, 65, 63, 66, 62, 56, 73, 60, 65, 81, 64,  1,\n",
            "        68, 56, 54, 60, 52,  7,  1, 69, 56, 54])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdxrxZea2Wud",
        "outputId": "0f5a0c03-e1d1-474b-ce85-cc68e57ae809"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([62, 60, 53,  ..., 32, 42,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Dividir en entrenamiento y validation set\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCKtoAdb2kVd",
        "outputId": "8021c71a-28d9-4cf5-e3a4-927b7e3d3415"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([27, 38, 37,  1, 40, 44, 32, 33, 38])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StTRGDLH3mX8",
        "outputId": "8b43cc18-4e7c-47ba-d2ab-806600d41987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuando el insumo es (x) tensor([27]) el objetivo (y): 38\n",
            "cuando el insumo es (x) tensor([27, 38]) el objetivo (y): 37\n",
            "cuando el insumo es (x) tensor([27, 38, 37]) el objetivo (y): 1\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1]) el objetivo (y): 40\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40]) el objetivo (y): 44\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40, 44]) el objetivo (y): 32\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40, 44, 32]) el objetivo (y): 33\n",
            "cuando el insumo es (x) tensor([27, 38, 37,  1, 40, 44, 32, 33]) el objetivo (y): 38\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"cuando el insumo es (x) {context} el objetivo (y): {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FCAVWIhx4MGB"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "block_size = 8 # tamaño de contexto\n",
        "batch_size = 4 # numero de secuencias en paralelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9zvZ6C1d4gY5"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFDmIybG4iBy",
        "outputId": "b945affd-f81d-4918-f294-bbc1395723c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[70, 52, 55,  1, 55, 56,  1, 66],\n",
            "        [ 7,  1, 54, 65, 64,  1, 56, 62],\n",
            "        [60,  1, 52,  1, 63, 60, 81,  1],\n",
            "        [54, 52, 68, 52,  7,  1, 64, 60]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[52, 55,  1, 55, 56,  1, 66, 52],\n",
            "        [ 1, 54, 65, 64,  1, 56, 62,  1],\n",
            "        [ 1, 52,  1, 63, 60, 81,  1, 63],\n",
            "        [52, 68, 52,  7,  1, 64, 60,  1]])\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkiy257w7LIr",
        "outputId": "8c650795-f275-44dc-d817-dbed8563c38c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b377ab2eaf0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Tjs5huYl5xiY"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kzd1qzTy7Dg4"
      },
      "outputs": [],
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "logits = m(xb, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_EAgaVm94vL",
        "outputId": "bc2e800a-edaa-4c5a-fa22-aff3be2641f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8sLv<.OTM»̃\"DK;C??c\"TŃ6)-HpLL);GhLRN!̀’IJ1PS«L¡X9”:l\";;]:5Vnd»PjYV“Y«\"ITaQ2’̀rE?x”b‘5”c¿o'H7L[cB3;Q\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DvbUU4lz_0uJ"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHMY3liO_3vI",
        "outputId": "f002adc1-2605-4ecc-d664-c4e4a3b1f852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.233574628829956\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4_RGLD1ARjQ",
        "outputId": "7ae954b2-5854-4723-e9b5-e2dd7eb5a727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N: caro, SAle dono Sa el ]Qunusa es sanzcompomi nte dida la yasueriela W¡cha tre eIAmo vondiece os do cos as horos, da hacise Q ja yen pe. anteta ta, s, pas lomes cárá micora; yobiesañodabrel, pé, ve sís al dí tenójo acovidrlllo e leno vira se sosan queguerigo, de ce –X dejon cor sna, ba durídenta l o; nsOre s scicotea labla dílo y EVX(Anazoronte pesalo, Dosabra desin te bíncú tadabustométadera cegurtená la po a biase deblane y, cahe, litapamprcimide brama e canena ale s si¿mon lo, \n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}